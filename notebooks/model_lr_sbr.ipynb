{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f4817d",
   "metadata": {},
   "source": [
    "# Definición del Problema\n",
    "## Objetivo: Clasificar segmentos de audio como \"música\" o \"no música\".\n",
    "\n",
    "## Métricas principales:\n",
    "- Precision (evitar falsos positivos en música, ej: no etiquetar ruido como música).\n",
    "- Recall (capturar la mayor cantidad de música real).\n",
    "- F1-score (balance entre ambas).\n",
    "\n",
    "## Dataset:\n",
    "- Número de ejemplos (14,661 no-música / 7,499 música).\n",
    "- Features: Embeddings de audio (shape [seq_length, 128])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bcfb9",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30dc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cda3c",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64a6abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Descarga local temporalmente ===\n",
    "balanced_train_segments_path = \"c:/Users/sbrxb/waveled/data/raw/balanced_train_segments.csv\"\n",
    "ontology_path = \"c:/Users/sbrxb/waveled/data/raw/ontology.json\"\n",
    "dir_bal_train = \"c:/Users/sbrxb/waveled/data/raw/bal_train\" \n",
    "dir_eval = \"c:/Users/sbrxb/waveled/data/raw/eval\" \n",
    "class_labels_indices_path = \"c:/Users/sbrxb/waveled/data/raw/class_labels_indices.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591619ea",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "196cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(balanced_train_segments_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Leer el archivo línea por línea y reconstruir las filas correctamente\n",
    "fixed_rows = []\n",
    "for line in lines[3:]:  \n",
    "    parts = line.strip().split(\",\")  \n",
    "    if len(parts) >= 4:  \n",
    "        fixed_rows.append([parts[0], parts[1], parts[2], \",\".join(parts[3:])]) \n",
    "\n",
    "# Crear un DataFrame\n",
    "df_segments = pd.DataFrame(fixed_rows, columns=[\"YTID\", \"start_seconds\", \"end_seconds\", \"positive_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "307582f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>citation_uri</th>\n",
       "      <th>positive_examples</th>\n",
       "      <th>child_ids</th>\n",
       "      <th>restrictions</th>\n",
       "      <th>is_music</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/0dgw9r</td>\n",
       "      <td>Human sounds</td>\n",
       "      <td>Sounds produced by the human body through the ...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[/m/09l8g, /m/01w250, /m/09hlz4, /m/0bpl036, /...</td>\n",
       "      <td>[abstract]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/09l8g</td>\n",
       "      <td>Human voice</td>\n",
       "      <td>The human voice consists of sound made by a hu...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Human_voice</td>\n",
       "      <td>[]</td>\n",
       "      <td>[/m/09x0r, /m/07p6fty, /m/03qc9zr, /m/02rtxlg,...</td>\n",
       "      <td>[abstract]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/09x0r</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech is the vocalized form of human communic...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Speech</td>\n",
       "      <td>[youtu.be/8uI9H5jGRV8?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[/m/05zppz, /m/02zsn, /m/0ytgt, /m/01h8n0, /m/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/05zppz</td>\n",
       "      <td>Male speech, man speaking</td>\n",
       "      <td>Speech uttered by an adult male human.</td>\n",
       "      <td></td>\n",
       "      <td>[youtu.be/6niRPYpLOpQ?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/02zsn</td>\n",
       "      <td>Female speech, woman speaking</td>\n",
       "      <td>Speech uttered by an adult female human.</td>\n",
       "      <td></td>\n",
       "      <td>[youtu.be/4l05nCOnIRg?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                           name  \\\n",
       "0  /m/0dgw9r                   Human sounds   \n",
       "1   /m/09l8g                    Human voice   \n",
       "2   /m/09x0r                         Speech   \n",
       "3  /m/05zppz      Male speech, man speaking   \n",
       "4   /m/02zsn  Female speech, woman speaking   \n",
       "\n",
       "                                         description  \\\n",
       "0  Sounds produced by the human body through the ...   \n",
       "1  The human voice consists of sound made by a hu...   \n",
       "2  Speech is the vocalized form of human communic...   \n",
       "3             Speech uttered by an adult male human.   \n",
       "4           Speech uttered by an adult female human.   \n",
       "\n",
       "                               citation_uri  \\\n",
       "0                                             \n",
       "1  http://en.wikipedia.org/wiki/Human_voice   \n",
       "2       http://en.wikipedia.org/wiki/Speech   \n",
       "3                                             \n",
       "4                                             \n",
       "\n",
       "                                   positive_examples  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [youtu.be/8uI9H5jGRV8?start=30&end=40, youtu.b...   \n",
       "3  [youtu.be/6niRPYpLOpQ?start=30&end=40, youtu.b...   \n",
       "4  [youtu.be/4l05nCOnIRg?start=30&end=40, youtu.b...   \n",
       "\n",
       "                                           child_ids restrictions  is_music  \n",
       "0  [/m/09l8g, /m/01w250, /m/09hlz4, /m/0bpl036, /...   [abstract]         0  \n",
       "1  [/m/09x0r, /m/07p6fty, /m/03qc9zr, /m/02rtxlg,...   [abstract]         0  \n",
       "2  [/m/05zppz, /m/02zsn, /m/0ytgt, /m/01h8n0, /m/...           []         0  \n",
       "3                                                 []           []         0  \n",
       "4                                                 []           []         0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el JSON ontology\n",
    "with open(ontology_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_ontology = pd.DataFrame(data)\n",
    "\n",
    "keywords = [\"music\", \"musical\", \"song\", \"instrument\", \"singing\"]\n",
    "\n",
    "# Crear expresión regular con OR entre palabras\n",
    "pattern = \"|\".join(keywords)\n",
    "\n",
    "# Crear la columna is_music basada en las palabras clave\n",
    "df_ontology[\"is_music\"] = df_ontology[\"name\"].str.lower().str.contains(pattern).astype(int)\n",
    "\n",
    "df_ontology.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "679a88eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mid</th>\n",
       "      <th>display_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/m/09x0r</td>\n",
       "      <td>Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/m/05zppz</td>\n",
       "      <td>Male speech, man speaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/m/02zsn</td>\n",
       "      <td>Female speech, woman speaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/m/0ytgt</td>\n",
       "      <td>Child speech, kid speaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/m/01h8n0</td>\n",
       "      <td>Conversation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        mid                   display_name\n",
       "0      0   /m/09x0r                         Speech\n",
       "1      1  /m/05zppz      Male speech, man speaking\n",
       "2      2   /m/02zsn  Female speech, woman speaking\n",
       "3      3   /m/0ytgt     Child speech, kid speaking\n",
       "4      4  /m/01h8n0                   Conversation"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el CSV en un DataFrame\n",
    "df_class_labels_indices = pd.read_csv(class_labels_indices_path)\n",
    "\n",
    "df_class_labels_indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05ae2be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>citation_uri</th>\n",
       "      <th>positive_examples</th>\n",
       "      <th>child_ids</th>\n",
       "      <th>restrictions</th>\n",
       "      <th>is_music</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/09x0r</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech is the vocalized form of human communic...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Speech</td>\n",
       "      <td>[youtu.be/8uI9H5jGRV8?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[/m/05zppz, /m/02zsn, /m/0ytgt, /m/01h8n0, /m/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/05zppz</td>\n",
       "      <td>Male speech, man speaking</td>\n",
       "      <td>Speech uttered by an adult male human.</td>\n",
       "      <td></td>\n",
       "      <td>[youtu.be/6niRPYpLOpQ?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/02zsn</td>\n",
       "      <td>Female speech, woman speaking</td>\n",
       "      <td>Speech uttered by an adult female human.</td>\n",
       "      <td></td>\n",
       "      <td>[youtu.be/4l05nCOnIRg?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/0ytgt</td>\n",
       "      <td>Child speech, kid speaking</td>\n",
       "      <td>Speech uttered by a human child, i.e. a human ...</td>\n",
       "      <td></td>\n",
       "      <td>[youtu.be/iPIGoScYduI?start=210&amp;end=220, youtu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/01h8n0</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>Interactive, spontaneous spoken communication ...</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Conversation</td>\n",
       "      <td>[youtu.be/4FQxw_49xAk?start=30&amp;end=40, youtu.b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                           name  \\\n",
       "index                                             \n",
       "0       /m/09x0r                         Speech   \n",
       "1      /m/05zppz      Male speech, man speaking   \n",
       "2       /m/02zsn  Female speech, woman speaking   \n",
       "3       /m/0ytgt     Child speech, kid speaking   \n",
       "4      /m/01h8n0                   Conversation   \n",
       "\n",
       "                                             description  \\\n",
       "index                                                      \n",
       "0      Speech is the vocalized form of human communic...   \n",
       "1                 Speech uttered by an adult male human.   \n",
       "2               Speech uttered by an adult female human.   \n",
       "3      Speech uttered by a human child, i.e. a human ...   \n",
       "4      Interactive, spontaneous spoken communication ...   \n",
       "\n",
       "                                    citation_uri  \\\n",
       "index                                              \n",
       "0            http://en.wikipedia.org/wiki/Speech   \n",
       "1                                                  \n",
       "2                                                  \n",
       "3                                                  \n",
       "4      http://en.wikipedia.org/wiki/Conversation   \n",
       "\n",
       "                                       positive_examples  \\\n",
       "index                                                      \n",
       "0      [youtu.be/8uI9H5jGRV8?start=30&end=40, youtu.b...   \n",
       "1      [youtu.be/6niRPYpLOpQ?start=30&end=40, youtu.b...   \n",
       "2      [youtu.be/4l05nCOnIRg?start=30&end=40, youtu.b...   \n",
       "3      [youtu.be/iPIGoScYduI?start=210&end=220, youtu...   \n",
       "4      [youtu.be/4FQxw_49xAk?start=30&end=40, youtu.b...   \n",
       "\n",
       "                                               child_ids restrictions  \\\n",
       "index                                                                   \n",
       "0      [/m/05zppz, /m/02zsn, /m/0ytgt, /m/01h8n0, /m/...           []   \n",
       "1                                                     []           []   \n",
       "2                                                     []           []   \n",
       "3                                                     []           []   \n",
       "4                                                     []           []   \n",
       "\n",
       "       is_music  \n",
       "index            \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge entre 'class_labels_indices' y 'ontology'\n",
    "df_ontology_labels = pd.merge(df_class_labels_indices, df_ontology, left_on='mid', right_on='id', how='left')\n",
    "\n",
    "# Eliminamos las columnas 'mid' y 'display_name'\n",
    "df_ontology_labels = df_ontology_labels.drop(columns=['mid', 'display_name'])\n",
    "\n",
    "# Diccionario con los índices como claves y las id como valores\n",
    "id_labels_dict = df_ontology_labels.set_index('index')['id'].to_dict()\n",
    "\n",
    "# Establecemos 'index' como índice\n",
    "df_ontology_labels.set_index('index', inplace=True)\n",
    "\n",
    "df_ontology_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d3fe9",
   "metadata": {},
   "source": [
    "# Funciones para Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4daa7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(reduce_retracing=True)\n",
    "def parse_music_example(example_proto, music_ids, id_labels_dict, seq_length=10):\n",
    "    \"\"\"\n",
    "    Procesa un ejemplo de música serializado en formato TFRecord.\n",
    "    \n",
    "    Args:\n",
    "        example_proto: Ejemplo serializado en formato TFRecord\n",
    "        music_ids: Tensor con los IDs de música que queremos detectar\n",
    "        id_labels_dict: Diccionario que mapea índices numéricos a IDs semánticos\n",
    "        seq_length: Longitud máxima de la secuencia de audio (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        audio_embeddings: Tensor con los embeddings de audio normalizados y paddeados [seq_length, 128]\n",
    "        is_music: Tensor float32 (0.0 o 1.0) indicando si el ejemplo contiene alguna música de interés\n",
    "    \"\"\"\n",
    "    # Convertir el diccionario a tensor constante\n",
    "    id_labels_tensor = tf.constant(list(id_labels_dict.values()))\n",
    "\n",
    "    # Definición de características del TFRecord\n",
    "    context_features = {\n",
    "        \"video_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    sequence_features = {\n",
    "        \"audio_embedding\": tf.io.FixedLenSequenceFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    # Parsear el ejemplo protobuf\n",
    "    context, sequences = tf.io.parse_single_sequence_example(\n",
    "        example_proto, context_features=context_features, sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    # Procesamiento de embeddings de audio\n",
    "    audio_embeddings = tf.io.decode_raw(sequences['audio_embedding'], tf.uint8)\n",
    "    audio_embeddings = tf.reshape(audio_embeddings, [-1, 128])\n",
    "    audio_embeddings = (tf.cast(audio_embeddings, tf.float32) - 127.5) / 127.5\n",
    "    audio_embeddings = audio_embeddings[:seq_length]\n",
    "    padding = [[0, seq_length - tf.shape(audio_embeddings)[0]], [0, 0]]\n",
    "    audio_embeddings = tf.pad(audio_embeddings, padding)\n",
    "    audio_embeddings.set_shape([seq_length, 128])\n",
    "\n",
    "    # Procesamiento de etiquetas\n",
    "    labels = tf.sparse.to_dense(context['labels'])\n",
    "\n",
    "    # Convertir labels a IDs y comparar con music_ids\n",
    "    id_labels = tf.gather(id_labels_tensor, labels)\n",
    "    \n",
    "    # Verificar si alguna etiqueta coincide con music_ids\n",
    "    is_music = tf.reduce_any(tf.equal(tf.expand_dims(id_labels, -1), music_ids))\n",
    "    \n",
    "    return audio_embeddings, tf.cast(is_music, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8e60edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecord_dir, music_ids, batch_size=32, seq_length=10, is_training=True):\n",
    "    \"\"\"\n",
    "    Crea un pipeline de datos TensorFlow configurable para entrenamiento o evaluación.\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_dir: Ruta al directorio con archivos .tfrecord\n",
    "        music_ids: Lista de IDs de música a detectar\n",
    "        batch_size: Tamaño del batch (default: 32)\n",
    "        seq_length: Longitud de secuencia para embeddings de audio (default: 10)\n",
    "        is_training: Bool (True para dataset de entrenamiento, False para test) (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset configurado con la estructura:\n",
    "        - audio_embeddings: [batch_size, seq_length, 128]\n",
    "        - is_music: [batch_size] (0.0 o 1.0)\n",
    "    \"\"\"\n",
    "    # Validación de archivos\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    if not tfrecord_files:\n",
    "        raise ValueError(f\"No se encontraron archivos .tfrecord en {tfrecord_dir}\")\n",
    "        \n",
    "    # Preprocesamiento de music_ids (convertir a tensor constante una sola vez)\n",
    "    music_ids_tensor = tf.constant([str(id) for id in music_ids], dtype=tf.string)\n",
    "    \n",
    "    # Crear dataset base desde los archivos TFRecord\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Definir y aplicar función de parseo\n",
    "    parse_fn = lambda x: parse_music_example(x, music_ids_tensor, id_labels_dict, seq_length)\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    '''\n",
    "    # Configuración específica por modo\n",
    "    if is_training:\n",
    "        # Aumento de datos (por revisar)\n",
    "    '''\n",
    "    \n",
    "    # Creación de batches\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Prefetch para ambos modos\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7a493",
   "metadata": {},
   "source": [
    "# Modelos y Ajuste de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "265fa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_ids = set(df_ontology_labels[df_ontology_labels[\"is_music\"] == 1][\"id\"].astype(str))\n",
    "\n",
    "# Crear dataset\n",
    "full_dataset = create_dataset(\n",
    "    tfrecord_dir=dir_bal_train,\n",
    "    music_ids=music_ids \n",
    ")\n",
    "\n",
    "# Tamaño del Dataset\n",
    "count = 0\n",
    "for batch in full_dataset:\n",
    "    count += len(batch)  # Tamaño real de cada batch\n",
    "    \n",
    "dataset_size = count\n",
    "\n",
    "# Dividir dataset\n",
    "val_size = int(.2 * dataset_size)\n",
    "train_ds = full_dataset.skip(val_size).shuffle(buffer_size=1000, seed=10)  \n",
    "val_ds = full_dataset.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d709264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir TF Dataset a arrays\n",
    "def dataset_to_numpy(dataset):\n",
    "    X, y = [], []\n",
    "    for audio_emb, label in dataset.unbatch():\n",
    "        X.append(audio_emb.numpy().flatten())  # Aplanar [seq_length, 128] a [seq_length * 128]\n",
    "        y.append(label.numpy())\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = dataset_to_numpy(train_ds)\n",
    "X_val, y_val = dataset_to_numpy(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a279a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c2241",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c86cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros (Regresión Logística): {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Búsqueda de hiperparámetros\n",
    "params_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10], \n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state=10)\n",
    "grid_lr = GridSearchCV(logreg, params_lr, cv=3, scoring='f1')\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Mejores parámetros (Regresión Logística):\", grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045c18a",
   "metadata": {},
   "source": [
    "## Máquinas de Vectores de Soporte (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm = SVC(class_weight='balanced', random_state=10)\n",
    "grid_svm = GridSearchCV(svm, params_svm, cv=3, scoring='f1')\n",
    "grid_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Mejores parámetros (SVM):\", grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744cd25",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=10)\n",
    "grid_rf = GridSearchCV(rf, params_rf, cv=3, scoring='f1')\n",
    "grid_rf.fit(X_train, y_train)  \n",
    "\n",
    "print(\"Mejores parámetros (Random Forest):\", grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3efed",
   "metadata": {},
   "source": [
    "## Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add610f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 6],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  \n",
    "    random_state=10\n",
    ")\n",
    "grid_xgb = GridSearchCV(xgb, params_xgb, cv=3, scoring='f1')\n",
    "grid_xgb.fit(X_train, y_train)  \n",
    "\n",
    "print(\"Mejores parámetros (XGBoost):\", grid_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7edd6",
   "metadata": {},
   "source": [
    "## Evaluación y Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Regresión Logística\": grid_lr.best_estimator_,\n",
    "    \"SVM\": grid_svm.best_estimator_,\n",
    "    \"Random Forest\": grid_rf.best_estimator_,\n",
    "    \"XGBoost\": grid_xgb.best_estimator_\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    if name in [\"Regresión Logística\", \"SVM\"]:\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = classification_report(y_val, y_pred, output_dict=True)['1']['precision']\n",
    "    recall = classification_report(y_val, y_pred, output_dict=True)['1']['recall']\n",
    "    results.append({\n",
    "        'Modelo': name,\n",
    "        'F1-score': round(f1, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4)\n",
    "    })\n",
    "\n",
    "# Resultados en tabla\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Regresión Logística\": grid_lr.best_estimator_,\n",
    "    \"SVM\": grid_svm.best_estimator_,\n",
    "    \"Random Forest\": grid_rf.best_estimator_,\n",
    "    \"XGBoost\": grid_xgb.best_estimator_\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "\n",
    "# Iterar sobre cada modelo\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    # Usar X_train_scaled para modelos que requieren escalado\n",
    "    X_train_plot = X_train_scaled if name in [\"Regresión Logística\", \"SVM\"] else X_train\n",
    "    \n",
    "    # Calcular curva de aprendizaje\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train_plot,\n",
    "        y=y_train,\n",
    "        cv=3,\n",
    "        scoring='f1',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Calcular media y desviación estándar\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.plot(train_sizes, train_mean, \n",
    "             color=colors[i], linestyle=linestyles[i], \n",
    "             marker='o', label=f'{name} (Train)')\n",
    "    plt.fill_between(train_sizes, \n",
    "                    train_mean - train_std,\n",
    "                    train_mean + train_std,\n",
    "                    alpha=0.1, color=colors[i])\n",
    "    \n",
    "    plt.plot(train_sizes, val_mean, \n",
    "             color=colors[i], linestyle=linestyles[i], \n",
    "             marker='s', label=f'{name} (Val)')\n",
    "    plt.fill_between(train_sizes,\n",
    "                    val_mean - val_std,\n",
    "                    val_mean + val_std,\n",
    "                    alpha=0.1, color=colors[i])\n",
    "\n",
    "plt.title('Curvas de Aprendizaje Comparativas', fontsize=14)\n",
    "plt.xlabel('Tamaño del Conjunto de Entrenamiento', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.ylim(0.5, 1.0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf5add",
   "metadata": {},
   "source": [
    "# Modelo regresión logística PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b77e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ejemplos: 1386\n",
      "Entrenamiento: 1109\n",
      "Validación: 277\n"
     ]
    }
   ],
   "source": [
    "music_ids = set(df_ontology_labels[df_ontology_labels[\"is_music\"] == 1][\"id\"].astype(str))\n",
    "\n",
    "# Crear dataset\n",
    "full_dataset = create_dataset(\n",
    "    tfrecord_dir=dir_bal_train,\n",
    "    music_ids=music_ids \n",
    ")\n",
    "\n",
    "# Tamaño del Dataset\n",
    "count = 0\n",
    "for batch in full_dataset:\n",
    "    count += len(batch)  # Tamaño real de cada batch\n",
    "    \n",
    "dataset_size = count\n",
    "\n",
    "# Dividir dataset\n",
    "val_size = int(.2 * dataset_size)\n",
    "train_ds = full_dataset.skip(val_size).shuffle(buffer_size=1000, seed=10)  \n",
    "val_ds = full_dataset.take(val_size)\n",
    "\n",
    "# Verificar tamaños\n",
    "print(f\"Total ejemplos: {dataset_size}\")\n",
    "print(f\"Entrenamiento: {dataset_size - val_size}\")\n",
    "print(f\"Validación: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81c1fd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbrxb\\waveled\\env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Construir modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(10, 128)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a83247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fee56711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Distribución de Clases ===\n",
      "Ejemplos positivos (música): 7499\n",
      "Ejemplos negativos (NO música): 14661\n",
      "\n",
      "Pesos de clase para balanceo: {0: 0.7557465384353046, 1: 1.477530337378317}\n"
     ]
    }
   ],
   "source": [
    "# Conteo de clases\n",
    "pos = 0\n",
    "neg = 0\n",
    "for _, y in full_dataset.unbatch():  \n",
    "    if y.numpy() == 1.0:\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1\n",
    "\n",
    "print(\"\\n=== Distribución de Clases ===\")\n",
    "print(f\"Ejemplos positivos (música): {pos}\")\n",
    "print(f\"Ejemplos negativos (NO música): {neg}\")\n",
    "\n",
    "total_samples = pos + neg \n",
    "class_weight = {\n",
    "    0: total_samples / (2 * neg),  # Peso para NO música\n",
    "    1: total_samples / (2 * pos)    # Peso para música\n",
    "}\n",
    "print(f\"\\nPesos de clase para balanceo: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e04bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    403/Unknown \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7664 - loss: 1.1845 - precision: 0.6291 - recall: 0.8257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbrxb\\waveled\\env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7681 - loss: 1.1735 - precision: 0.6310 - recall: 0.8259 - val_accuracy: 0.8395 - val_loss: 0.5433 - val_precision: 0.7312 - val_recall: 0.8457\n",
      "Epoch 2/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8565 - loss: 0.4913 - precision: 0.7471 - recall: 0.8649 - val_accuracy: 0.8447 - val_loss: 0.4913 - val_precision: 0.7414 - val_recall: 0.8444\n",
      "Epoch 3/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8593 - loss: 0.4563 - precision: 0.7524 - recall: 0.8657 - val_accuracy: 0.8430 - val_loss: 0.4956 - val_precision: 0.7411 - val_recall: 0.8375\n",
      "Epoch 4/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8587 - loss: 0.4519 - precision: 0.7481 - recall: 0.8764 - val_accuracy: 0.8450 - val_loss: 0.4837 - val_precision: 0.7444 - val_recall: 0.8388\n",
      "Epoch 5/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.4456 - precision: 0.7539 - recall: 0.8845 - val_accuracy: 0.8465 - val_loss: 0.4775 - val_precision: 0.7454 - val_recall: 0.8431\n",
      "Epoch 6/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8552 - loss: 0.4465 - precision: 0.7376 - recall: 0.8769 - val_accuracy: 0.8347 - val_loss: 0.4974 - val_precision: 0.7232 - val_recall: 0.8441\n",
      "Epoch 7/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.4340 - precision: 0.7519 - recall: 0.8889 - val_accuracy: 0.8365 - val_loss: 0.5037 - val_precision: 0.7204 - val_recall: 0.8601\n",
      "Epoch 8/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8638 - loss: 0.4365 - precision: 0.7397 - recall: 0.8977 - val_accuracy: 0.8427 - val_loss: 0.5006 - val_precision: 0.7401 - val_recall: 0.8388\n",
      "Epoch 9/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.4321 - precision: 0.7519 - recall: 0.8864 - val_accuracy: 0.8294 - val_loss: 0.5096 - val_precision: 0.7107 - val_recall: 0.8529\n",
      "Epoch 10/10\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8671 - loss: 0.4275 - precision: 0.7567 - recall: 0.8939 - val_accuracy: 0.8447 - val_loss: 0.5046 - val_precision: 0.7393 - val_recall: 0.8493\n"
     ]
    }
   ],
   "source": [
    "# Entrenar\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    class_weight=class_weight \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5b8e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset de test\n",
    "test_ds = create_dataset(\n",
    "    tfrecord_dir=dir_eval,  \n",
    "    music_ids=music_ids,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ba229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo según ROC: 0.4768\n",
      "\n",
      "=== Métricas con Umbral Óptimo ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No música       0.92      0.83      0.87     13612\n",
      "      Música       0.71      0.85      0.77      6759\n",
      "\n",
      "    accuracy                           0.84     20371\n",
      "   macro avg       0.82      0.84      0.82     20371\n",
      "weighted avg       0.85      0.84      0.84     20371\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[11313  2299]\n",
      " [ 1029  5730]]\n",
      "\n",
      "=== Métricas con Umbral=0.5 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No música       0.91      0.84      0.87     13612\n",
      "      Música       0.72      0.84      0.77      6759\n",
      "\n",
      "    accuracy                           0.84     20371\n",
      "   macro avg       0.82      0.84      0.82     20371\n",
      "weighted avg       0.85      0.84      0.84     20371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_probs = []  \n",
    "\n",
    "for x, y in test_ds.unbatch():\n",
    "    y_true.append(y.numpy())\n",
    "    y_probs.append(model.predict(tf.expand_dims(x, axis=0), verbose=0)[0][0])\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# Umbral óptimo\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "optimal_idx = np.argmax(tpr - fpr)  \n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"\\nUmbral óptimo según ROC: {optimal_threshold:.4f}\")\n",
    "\n",
    "y_pred_optimal = (y_probs > optimal_threshold).astype(int)\n",
    "\n",
    "# Métricas con el nuevo umbral\n",
    "print(\"\\n=== Métricas con Umbral Óptimo ===\")\n",
    "print(classification_report(y_true, y_pred_optimal, target_names=[\"No música\", \"Música\"]))\n",
    "\n",
    "# Comparación con el umbral por defecto (0.5)\n",
    "print(\"\\n=== Métricas con Umbral=0.5 ===\")\n",
    "print(classification_report(y_true, (y_probs > 0.5).astype(int), target_names=[\"No música\", \"Música\"]))\n",
    "\n",
    "# Matriz de confusión\n",
    "conf_mat = confusion_matrix(y_true, y_pred_optimal)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicción: No música', 'Predicción: Música'],\n",
    "            yticklabels=['Real: No música', 'Real: Música'])\n",
    "plt.title('Matriz de Confusión - Umbral Óptimo')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()\n",
    "\n",
    "# Para el umbral por defecto (0.5)\n",
    "conf_mat_default = confusion_matrix(y_true, (y_probs > 0.5).astype(int))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat_default, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicción: No música', 'Predicción: Música'],\n",
    "            yticklabels=['Real: No música', 'Real: Música'])\n",
    "plt.title('Matriz de Confusión - Umbral=0.5')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
