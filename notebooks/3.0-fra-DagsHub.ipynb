{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e345c808",
   "metadata": {},
   "source": [
    "Registro de Modelos en DagsHub - Audio Classification\n",
    "\n",
    "Este notebook entrena y registra modelos de clasificaci√≥n de audio en DagsHub, incluyendo:\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Classifier\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db1aeb",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d811b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import dagshub\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from requests.exceptions import RequestException\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "import tensorflow as tf\n",
    "import mlflow.sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5fcb1",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d488ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data directory\n",
    "base_dir = '../data/'\n",
    "\n",
    "# raw data directory\n",
    "raw_dir = os.path.join(base_dir, 'raw/')\n",
    "\n",
    "# interim data directory\n",
    "interim_dir = os.path.join(base_dir, 'interim/')\n",
    "\n",
    "# processed data directory\n",
    "proc_dir = os.path.join(base_dir, 'processed/')\n",
    "\n",
    "# splits \n",
    "splits = ['bal_train/', 'eval/', 'unbal_train/']\n",
    "\n",
    "# segments files\n",
    "segments = ['balanced_train_segments.csv', 'unbalanced_train_segments.csv', 'eval_segments.csv']\n",
    "\n",
    "# labels file path\n",
    "labels_path = os.path.join(raw_dir, 'class_labels_indices.csv')\n",
    "\n",
    "# ontology file path\n",
    "ont_path = os.path.join(raw_dir, 'ontology.json')\n",
    "\n",
    "# [raw/interim/processed][split]\n",
    "data_path = {'raw': {}, 'interim': {}, 'processed': {}}  \n",
    "data_dir = {'raw': {}, 'interim': {}, 'processed': {}}  \n",
    "\n",
    "for i, seg in enumerate(splits):\n",
    "    seg_rm = seg.replace('/', '')\n",
    "    \n",
    "    raw = os.path.join(raw_dir, splits[i])\n",
    "    data_dir['raw'][seg_rm] = raw\n",
    "    raw = os.path.join(raw, segments[i])\n",
    "    data_path['raw'][seg_rm] = raw\n",
    "    \n",
    "    interim = os.path.join(interim_dir, splits[i])\n",
    "    data_dir['interim'][seg_rm] = interim\n",
    "    interim = os.path.join(interim, segments[i])\n",
    "    data_path['interim'][seg_rm] = interim\n",
    "    \n",
    "    processed = os.path.join(proc_dir, splits[i])\n",
    "    data_dir['processed'][seg_rm] = processed\n",
    "    processed = os.path.join(processed, segments[i])\n",
    "    data_path['processed'][seg_rm] = processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63095dc",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(reduce_retracing=True)\n",
    "def parse_music_example(example_proto, music_ids, id_labels_dict, seq_length=10):\n",
    "    \"\"\"Parse TFRecord example.\"\"\"\n",
    "    # Convert dict to constant tensor\n",
    "    id_labels_tensor = tf.constant(list(id_labels_dict.values()))\n",
    "\n",
    "    # Define features\n",
    "    context_features = {\n",
    "        \"video_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    sequence_features = {\n",
    "        \"audio_embedding\": tf.io.FixedLenSequenceFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    # Parse the example\n",
    "    context, sequences = tf.io.parse_single_sequence_example(\n",
    "        example_proto, \n",
    "        context_features=context_features, \n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    # Process audio embeddings\n",
    "    audio_embeddings = tf.io.decode_raw(sequences['audio_embedding'], tf.uint8)\n",
    "    audio_embeddings = tf.reshape(audio_embeddings, [-1, 128])\n",
    "    audio_embeddings = (tf.cast(audio_embeddings, tf.float32) - 127.5) / 127.5\n",
    "    audio_embeddings = audio_embeddings[:seq_length]\n",
    "    padding = [[0, seq_length - tf.shape(audio_embeddings)[0]], [0, 0]]\n",
    "    audio_embeddings = tf.pad(audio_embeddings, padding)\n",
    "    audio_embeddings.set_shape([seq_length, 128])\n",
    "\n",
    "    # Process labels\n",
    "    labels = tf.sparse.to_dense(context['labels'])\n",
    "    id_labels = tf.gather(id_labels_tensor, labels)\n",
    "    \n",
    "    # Check if any label matches music_ids\n",
    "    is_music = tf.reduce_any(tf.equal(tf.expand_dims(id_labels, -1), music_ids))\n",
    "    \n",
    "    return audio_embeddings, tf.cast(is_music, tf.float32)\n",
    "\n",
    "def create_dataset(tfrecord_dir, music_ids, batch_size=32, seq_length=10, is_training=True):\n",
    "    \"\"\"Create TF dataset pipeline.\"\"\"\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    if not tfrecord_files:\n",
    "        raise ValueError(f\"No TFRecord files found in {tfrecord_dir}\")\n",
    "        \n",
    "    # Convert music_ids to tensor\n",
    "    music_ids_tensor = tf.constant([str(id) for id in music_ids], dtype=tf.string)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Parse examples\n",
    "    parse_fn = lambda x: parse_music_example(x, music_ids_tensor, id_labels_dict, seq_length)\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Shuffle if training\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Convert TF Dataset to numpy arrays\n",
    "def dataset_to_numpy(dataset):\n",
    "    X, y = [], []\n",
    "    for audio_emb, label in dataset.unbatch():\n",
    "        X.append(audio_emb.numpy().flatten())  # Flatten to [seq_length * 128]\n",
    "        y.append(label.numpy())\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def enhance_linear_svc(model):\n",
    "    \"\"\"Verifies if LinearSVC is already calibrated, wraps if needed\"\"\"\n",
    "    if isinstance(model, LinearSVC):\n",
    "        print(\"‚ö†Ô∏è Found uncalibrated LinearSVC - adding calibration (this may not work well)\")\n",
    "        return CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "    return model\n",
    "\n",
    "def train_and_register_model(model, model_name, params, X_train, y_train, X_val, y_val, use_pca=False, calibrate=False):\n",
    "    \"\"\"Train model with GridSearchCV and log to MLflow.\"\"\"\n",
    "    run_name = f\"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"dataset\", \"AudioSet\")\n",
    "        mlflow.set_tag(\"features\", \"PCA\" if use_pca else \"Original\")\n",
    "\n",
    "        # Special handling for models that need calibration\n",
    "        if calibrate:\n",
    "            print(\"‚öôÔ∏è Setting up calibration pipeline...\")\n",
    "            # First find best LinearSVC parameters\n",
    "            svc_grid = GridSearchCV(\n",
    "                model,\n",
    "                params,\n",
    "                cv=3,\n",
    "                scoring='f1_weighted',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            svc_grid.fit(X_train, y_train)\n",
    "            \n",
    "            # Then calibrate the best model\n",
    "            calibrated_model = CalibratedClassifierCV(\n",
    "                svc_grid.best_estimator_,\n",
    "                method='sigmoid',\n",
    "                cv='prefit'\n",
    "            )\n",
    "            calibrated_model.fit(X_train, y_train)\n",
    "            best_model = calibrated_model\n",
    "            best_params = svc_grid.best_params_\n",
    "        else:\n",
    "            # Standard GridSearchCV for other models\n",
    "            grid = GridSearchCV(\n",
    "                model,\n",
    "                params,\n",
    "                cv=3,\n",
    "                scoring='f1_weighted',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            best_params = grid.best_params_\n",
    "\n",
    "        # Special handling for XGBoost\n",
    "        if model_name == \"XGBoost\":\n",
    "            xgb_final = XGBClassifier(\n",
    "                **best_params,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),\n",
    "                random_state=10,\n",
    "                eval_metric='logloss',\n",
    "                early_stopping_rounds=10\n",
    "            )\n",
    "            xgb_final.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            best_model = xgb_final\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")\n",
    "\n",
    "        # Log the model\n",
    "        if model_name == \"XGBoost\":\n",
    "            mlflow.xgboost.log_model(\n",
    "                xgb_model=best_model,\n",
    "                artifact_path=model_name,\n",
    "                input_example=X_train[:1].copy(),\n",
    "                registered_model_name=model_name\n",
    "            )\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=best_model,\n",
    "                artifact_path=model_name,\n",
    "                input_example=X_train[:1].copy(), \n",
    "                registered_model_name=model_name\n",
    "            )\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics and params\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        print(f\"‚úÖ {model_name} - F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd921dbd",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25cd7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read balanced train segments\n",
    "with open(data_path['raw']['bal_train'], \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "fixed_rows = []\n",
    "for line in lines[3:]:  \n",
    "    parts = line.strip().split(\",\")  \n",
    "    if len(parts) >= 4:  \n",
    "        fixed_rows.append([parts[0], parts[1], parts[2], \",\".join(parts[3:])]) \n",
    "\n",
    "df_segments = pd.DataFrame(fixed_rows, columns=[\"YTID\", \"start_seconds\", \"end_seconds\", \"positive_labels\"])\n",
    "\n",
    "# Load ontology\n",
    "with open(ont_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_ontology = pd.DataFrame(data)\n",
    "\n",
    "# Create music labels\n",
    "keywords_column_name = [\"music\", \"musical\", \"song\", \"instrument\", \"singing\"]\n",
    "keywords_column_description = [\"music\", \"musical\", \"song\", \"singing\"]\n",
    "\n",
    "pattern_column_name = \"|\".join(keywords_column_name)\n",
    "pattern_column_description = \"|\".join(keywords_column_description)\n",
    "\n",
    "name_contains = df_ontology[\"name\"].str.lower().str.contains(pattern_column_name)\n",
    "description_contains = df_ontology[\"description\"].str.lower().str.contains(pattern_column_description, na=False)\n",
    "\n",
    "df_ontology[\"is_music\"] = (name_contains | description_contains).astype(int)\n",
    "\n",
    "# Load class labels\n",
    "df_class_labels_indices = pd.read_csv(labels_path)\n",
    "\n",
    "# Merge ontology and labels\n",
    "df_ontology_labels = pd.merge(df_class_labels_indices, df_ontology, left_on='mid', right_on='id', how='left')\n",
    "df_ontology_labels = df_ontology_labels.drop(columns=['mid', 'display_name'])\n",
    "id_labels_dict = df_ontology_labels.set_index('index')['id'].to_dict()\n",
    "df_ontology_labels.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cdf597",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4386d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get music IDs\n",
    "music_ids = set(df_ontology_labels[df_ontology_labels[\"is_music\"] == 1][\"id\"].astype(str))\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = create_dataset(\n",
    "    tfrecord_dir=data_dir['raw']['bal_train'],\n",
    "    music_ids=music_ids,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Better way to get dataset size (without loading all data)\n",
    "def get_dataset_size(dataset):\n",
    "    return sum(1 for _ in dataset)\n",
    "\n",
    "# Split dataset\n",
    "dataset_size = get_dataset_size(full_dataset)\n",
    "val_size = int(0.2 * dataset_size)\n",
    "train_ds = full_dataset.skip(val_size)\n",
    "val_ds = full_dataset.take(val_size)\n",
    "\n",
    "X_train, y_train = dataset_to_numpy(train_ds)\n",
    "X_val, y_val = dataset_to_numpy(val_ds)\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler().fit(X_train)  # Original scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Dimensionality reduction\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "\n",
    "# Normalization for PCA space\n",
    "scaler_pca = StandardScaler().fit(X_train_pca)  \n",
    "X_train_pca = scaler_pca.transform(X_train_pca)\n",
    "X_val_pca = scaler_pca.transform(X_val_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b8d9e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1e12c",
   "metadata": {},
   "source": [
    "### DagsHub init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5941b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DagsHub and MLflow\n",
    "try:\n",
    "    mlflow.set_tracking_uri(\"https://dagshub.com/Febas-MCD/waveled.mlflow\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to connect to remote tracking: {str(e)}\")\n",
    "    mlflow.set_tracking_uri(\"file://./mlruns\")\n",
    "    print(\"‚ö†Ô∏è Falling back to local MLflow tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6408340",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd321ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations with enhanced XGBoost settings\n",
    "models_config = [\n",
    "    {\n",
    "        \"model\": LogisticRegression(class_weight='balanced', random_state=10, max_iter=1000),\n",
    "        \"name\": \"LogisticRegression_PCA\",\n",
    "        \"params\": {\n",
    "            'C': [0.01, 0.1, 1, 10], \n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        },\n",
    "        \"use_pca\": True\n",
    "    },\n",
    "    {\n",
    "        \"model\": LinearSVC(class_weight='balanced', random_state=10, dual=False, max_iter=10000),\n",
    "        \"name\": \"LinearSVC_PCA\",\n",
    "        \"params\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'loss': ['squared_hinge']\n",
    "        },\n",
    "        \"use_pca\": True,\n",
    "        \"calibrate\": True \n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(class_weight='balanced', random_state=10),\n",
    "        \"name\": \"RandomForest\",\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        \"use_pca\": False\n",
    "    },\n",
    "    {\n",
    "        \"model\": XGBClassifier(\n",
    "            scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),\n",
    "            random_state=10,\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        \"name\": \"XGBoost\",\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 6],\n",
    "            'subsample': [0.8, 1.0],\n",
    "        },\n",
    "        \"use_pca\": False\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d67c7",
   "metadata": {},
   "source": [
    "### Model training and registering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7663b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fab77f0c8047e5ab0e920b1c579211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LogisticRegression_PCA' already exists. Creating a new version of this model...\n",
      "2025/05/17 21:46:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression_PCA, version 2\n",
      "Created version '2' of model 'LogisticRegression_PCA'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LogisticRegression_PCA - F1: 0.8590, Accuracy: 0.8582\n",
      "üèÉ View run LogisticRegression_PCA_20250517_214553 at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0/runs/2bd01ee707044c6fa3c424f263b3aea4\n",
      "üß™ View experiment at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf95627bcdf64c73af384cb43658f58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LinearSVC_PCA' already exists. Creating a new version of this model...\n",
      "2025/05/17 21:46:59 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LinearSVC_PCA, version 2\n",
      "Created version '2' of model 'LinearSVC_PCA'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LinearSVC_PCA - F1: 0.8585, Accuracy: 0.8580\n",
      "üèÉ View run LinearSVC_PCA_20250517_214640 at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0/runs/38c2ad4632f14dcaa08cce4deab2b7d2\n",
      "üß™ View experiment at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1e33592a6f4ca8b23e3a021aeda02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'RandomForest' already exists. Creating a new version of this model...\n",
      "2025/05/17 21:49:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: RandomForest, version 2\n",
      "Created version '2' of model 'RandomForest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RandomForest - F1: 0.8614, Accuracy: 0.8621\n",
      "üèÉ View run RandomForest_20250517_214701 at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0/runs/ebad1b80510242e889ec5b36b3d053a1\n",
      "üß™ View experiment at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1cba8abee3405fb52cf20da5e3e7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'XGBoost' already exists. Creating a new version of this model...\n",
      "2025/05/17 21:58:55 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: XGBoost, version 2\n",
      "Created version '2' of model 'XGBoost'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost - F1: 0.8756, Accuracy: 0.8757\n",
      "üèÉ View run XGBoost_20250517_214912 at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0/runs/b5606b62df26453cbe54635618cc9a42\n",
      "üß™ View experiment at: https://dagshub.com/Febas-MCD/waveled.mlflow/#/experiments/0\n",
      "üèÅ Training process completed\n"
     ]
    }
   ],
   "source": [
    "# Train models with fallback to local tracking if remote fails\n",
    "try:\n",
    "    for config in models_config:\n",
    "        train_data = X_train_pca if config[\"use_pca\"] else X_train_scaled\n",
    "        val_data = X_val_pca if config[\"use_pca\"] else X_val_scaled\n",
    "        \n",
    "        train_and_register_model(\n",
    "            model=config[\"model\"],\n",
    "            model_name=config[\"name\"],\n",
    "            params=config[\"params\"],\n",
    "            X_train=train_data,\n",
    "            y_train=y_train,\n",
    "            X_val=val_data,\n",
    "            y_val=y_val,\n",
    "            use_pca=config[\"use_pca\"]\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Critical error: {str(e)}\")\n",
    "finally:\n",
    "    print(\"üèÅ Training process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84af072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objetos guardados:\n",
      "['pca.joblib', 'scaler.joblib', 'scaler_pca.joblib']\n"
     ]
    }
   ],
   "source": [
    "# Crear directorio para modelos si no existe\n",
    "os.makedirs('model_objects', exist_ok=True)\n",
    "\n",
    "# Guardar todos los objetos necesarios\n",
    "joblib.dump(scaler, 'model_objects/scaler.joblib')  # StandardScaler\n",
    "joblib.dump(pca, 'model_objects/pca.joblib')        # PCA\n",
    "joblib.dump(scaler_pca, 'model_objects/scaler_pca.joblib')  # Scaler para PCA\n",
    "\n",
    "# Verificar que se guardaron correctamente\n",
    "print(\"Objetos guardados:\")\n",
    "print(os.listdir('model_objects'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
